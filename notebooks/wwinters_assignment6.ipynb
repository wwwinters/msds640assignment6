{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------\n",
    "- Wiley Winters\n",
    "- MSDS 640 - Assignment 6 Social Media Analysis for the Common Good\n",
    "- 2025-AUG-16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "# Assignment Specification\n",
    "For this week's assignment, you are tasked with write report APA-formatted paper (3-4 pages long). Assume the role of a data science researcher employed at a non-profit organization, approaching the topic from a data science's perspective.\n",
    "- Your main objective is to utilize social media data to contribute to a common good issue. Choose a topic such as **mental health**, **income inequality**, **human rights**, **workers' rights**, a particular **healthcare concern**, or **socioeconomic injustices**. Select a social media platform for data collection, and options include Reddit, X (formerly Twitter), Facebook, or others\n",
    "- At a minimum create the wordcloud and include it in your paper. To go above and beyond, apply other NLP and text analytics techniques, such as [topic modeling](https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0) and sentiment analysis. Note that many people consider wordclouds uninformative and [bad practice](https://getthematic.com/insights/word-clouds-harm-insights/), so you should strive to create a bar chart of top wordcloud or other visualizations instead, which can be done using tools and examples provided in resources like \"MSDS640_Week6_FTE.ipynb\"\n",
    "- Your paper should also feature a mindmap. This mindmap should center around the common good issue you have selected, with social media platforms branching out from the center. Further layers can delve into ethics and privacy concerns related to the project, culminating in examples of these issues\n",
    "- In your work, include an overall ***introduction***, a ***description of your dataset***, the ***purpose** behind your research, highlighting the problem you seek to address, and a discussion on ethics and privacy challenges in the context of your chosen common good issue. Additionally, provide a summary of your findings. For further insights and inspiration, refer to the weekly reading list, which includes videos and mind-mapping resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------\n",
    "## Import Required Packages and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Imports\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Read sqlite3 database file\n",
    "import sqlite3\n",
    "\n",
    "# Text processing and preparation\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords, words\n",
    "\n",
    "# Text Visualization\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Sentiment Analysis\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Make plots pretty\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process text function to perform basic preprocessing on text features.  I changed this function to use lemmatization instead of stemming.  While stemming can be faster to perform, lemmatization actually reduces the word-forms to linguistically valid lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower() # Convert all to lower case\n",
    "        # Remove punctuation\n",
    "        text = ''.join([char for char in text if char not in string.punctuation])\n",
    "        text = ''.join([char for char in text if not char.isdigit()])  # Remove numbers\n",
    "        # In some instances, I've run into issues with extra spaces.\n",
    "        text = text.strip()\n",
    "        # Remove stop words and apply lemmatizer\n",
    "        stop = stopwords.words('english')\n",
    "        wnl = WordNetLemmatizer()\n",
    "        text = ' '.join([wnl.lemmatize(word) for word in text.split() if word not in \\\n",
    "                         stop])\n",
    "\n",
    "        return text\n",
    "    else:\n",
    "        return ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to download **Reddit** submissions and their associated comments.  Since the relationship between submissions and comments are one to many, I decided to store the data in a *sqlite3* datafile.  This will allow me to query the data using standard SQL statements that allows for one to many SQL joins.\n",
    "\n",
    "The code below creates a connection to the datafile and then queries it based on criteria I have chosen.  A pandas dataframe is created from the query.\n",
    "\n",
    "Since I believe the purpose of the lab is to perform a sentiment analysis, I will only load text features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>n_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>ratio</th>\n",
       "      <th>text</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_utc</th>\n",
       "      <th>comment_author</th>\n",
       "      <th>body</th>\n",
       "      <th>comment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7164</th>\n",
       "      <td>16b2f62</td>\n",
       "      <td>1.693953e+09</td>\n",
       "      <td>If I ever get rich</td>\n",
       "      <td>TheAlmightyBuddha</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.90</td>\n",
       "      <td>If I ever get rich, I'll probably become the f...</td>\n",
       "      <td>n0irvib</td>\n",
       "      <td>1.751255e+09</td>\n",
       "      <td>TheAlmightyBuddha</td>\n",
       "      <td>doing a bit better now except I'm unemployed a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2668</th>\n",
       "      <td>1mh3nii</td>\n",
       "      <td>1.754281e+09</td>\n",
       "      <td>I think a lot of us can survive on Costco hot ...</td>\n",
       "      <td>Apprehensive_Name445</td>\n",
       "      <td>220</td>\n",
       "      <td>323</td>\n",
       "      <td>0.92</td>\n",
       "      <td>I have been going to these places everyday and...</td>\n",
       "      <td>n7hznly</td>\n",
       "      <td>1.754605e+09</td>\n",
       "      <td>Old-Set78</td>\n",
       "      <td>In Texas the ONLY thing that is required in re...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6319</th>\n",
       "      <td>1ml2agb</td>\n",
       "      <td>1.754676e+09</td>\n",
       "      <td>How to furnish apartment on a budget?</td>\n",
       "      <td>kscruggs182</td>\n",
       "      <td>48</td>\n",
       "      <td>12</td>\n",
       "      <td>1.00</td>\n",
       "      <td>I moved out when I was sixteen but managed to ...</td>\n",
       "      <td>n7r9dr0</td>\n",
       "      <td>1.754736e+09</td>\n",
       "      <td>WillGrahamsass</td>\n",
       "      <td>Dollar Tree, Five Below</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5280</th>\n",
       "      <td>1e8hq0o</td>\n",
       "      <td>1.721551e+09</td>\n",
       "      <td>Found motivation in hunger</td>\n",
       "      <td>soapsNjncojeans</td>\n",
       "      <td>14</td>\n",
       "      <td>25</td>\n",
       "      <td>0.88</td>\n",
       "      <td>First time in my life I don't have food or mon...</td>\n",
       "      <td>lhk3qyn</td>\n",
       "      <td>1.723362e+09</td>\n",
       "      <td>soapsNjncojeans</td>\n",
       "      <td>Copy that. I'm not one to complain but I'm als...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1650</th>\n",
       "      <td>1m3jg9t</td>\n",
       "      <td>1.752887e+09</td>\n",
       "      <td>The struggle is real.</td>\n",
       "      <td>Apprehensive_Snow45</td>\n",
       "      <td>273</td>\n",
       "      <td>876</td>\n",
       "      <td>0.98</td>\n",
       "      <td>I'm fresh out of prison and I'm feeling overwh...</td>\n",
       "      <td>n4nfz24</td>\n",
       "      <td>1.753242e+09</td>\n",
       "      <td>Brilliant-Hope691</td>\n",
       "      <td>Check with your school, they often have an IT ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id   created_utc  \\\n",
       "7164  16b2f62  1.693953e+09   \n",
       "2668  1mh3nii  1.754281e+09   \n",
       "6319  1ml2agb  1.754676e+09   \n",
       "5280  1e8hq0o  1.721551e+09   \n",
       "1650  1m3jg9t  1.752887e+09   \n",
       "\n",
       "                                                  title                author  \\\n",
       "7164                                 If I ever get rich     TheAlmightyBuddha   \n",
       "2668  I think a lot of us can survive on Costco hot ...  Apprehensive_Name445   \n",
       "6319              How to furnish apartment on a budget?           kscruggs182   \n",
       "5280                         Found motivation in hunger       soapsNjncojeans   \n",
       "1650                              The struggle is real.   Apprehensive_Snow45   \n",
       "\n",
       "      n_comments  score  ratio  \\\n",
       "7164           3      7   0.90   \n",
       "2668         220    323   0.92   \n",
       "6319          48     12   1.00   \n",
       "5280          14     25   0.88   \n",
       "1650         273    876   0.98   \n",
       "\n",
       "                                                   text comment_id  \\\n",
       "7164  If I ever get rich, I'll probably become the f...    n0irvib   \n",
       "2668  I have been going to these places everyday and...    n7hznly   \n",
       "6319  I moved out when I was sixteen but managed to ...    n7r9dr0   \n",
       "5280  First time in my life I don't have food or mon...    lhk3qyn   \n",
       "1650  I'm fresh out of prison and I'm feeling overwh...    n4nfz24   \n",
       "\n",
       "       comment_utc     comment_author  \\\n",
       "7164  1.751255e+09  TheAlmightyBuddha   \n",
       "2668  1.754605e+09          Old-Set78   \n",
       "6319  1.754736e+09     WillGrahamsass   \n",
       "5280  1.723362e+09    soapsNjncojeans   \n",
       "1650  1.753242e+09  Brilliant-Hope691   \n",
       "\n",
       "                                                   body  comment_score  \n",
       "7164  doing a bit better now except I'm unemployed a...              1  \n",
       "2668  In Texas the ONLY thing that is required in re...              3  \n",
       "6319                            Dollar Tree, Five Below              1  \n",
       "5280  Copy that. I'm not one to complain but I'm als...              1  \n",
       "1650  Check with your school, they often have an IT ...              2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = sqlite3.connect('data/poverty.sqlite')  # Create the database connection object\n",
    "\n",
    "reddit_df = pd.read_sql_query('SELECT id, created_utc, title, author, n_comments, ' \\\n",
    "                              'score, ratio, text, comment_id, comment_utc, ' \\\n",
    "                              'comment_author, body, comment_score ' \\\n",
    "                              'FROM posts, comments WHERE posts.id = comments.link_id ' \\\n",
    "                              'AND n_comments > 0', conn)\n",
    "# Take a quick peek at the data\n",
    "reddit_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dates are in Unix Epoch.  I will convert them into something a little more human readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>n_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>ratio</th>\n",
       "      <th>text</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_utc</th>\n",
       "      <th>comment_author</th>\n",
       "      <th>body</th>\n",
       "      <th>comment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2860</th>\n",
       "      <td>1lzybaq</td>\n",
       "      <td>2025-07-14 20:56:16</td>\n",
       "      <td>How are you guys affording everyday basic hygi...</td>\n",
       "      <td>MoodyMagicOwl</td>\n",
       "      <td>154</td>\n",
       "      <td>250</td>\n",
       "      <td>0.99</td>\n",
       "      <td>The title pretty much explains itself.\\n\\n I a...</td>\n",
       "      <td>n35efx3</td>\n",
       "      <td>2025-07-14 21:24:01</td>\n",
       "      <td>Carrie_1968</td>\n",
       "      <td>I do this. Once you get used to it you can *fe...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7660</th>\n",
       "      <td>197zvaf</td>\n",
       "      <td>2024-01-16 10:32:31</td>\n",
       "      <td>prescriptions</td>\n",
       "      <td>Illustrious-Level858</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.75</td>\n",
       "      <td>how to get free script help</td>\n",
       "      <td>kibctie</td>\n",
       "      <td>2024-01-17 18:27:33</td>\n",
       "      <td>kkaavvbb</td>\n",
       "      <td>Try calling the drug company. They typically o...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785</th>\n",
       "      <td>1mbceud</td>\n",
       "      <td>2025-07-28 10:18:40</td>\n",
       "      <td>Poor people can't have both kids and money</td>\n",
       "      <td>Grouchy_Key4343</td>\n",
       "      <td>271</td>\n",
       "      <td>1160</td>\n",
       "      <td>0.96</td>\n",
       "      <td>The way I see things, if you were born poor an...</td>\n",
       "      <td>n5muot7</td>\n",
       "      <td>2025-07-28 16:33:35</td>\n",
       "      <td>TheMaze01</td>\n",
       "      <td>And you've sought no remedy over the decades?</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3244</th>\n",
       "      <td>1kl71d8</td>\n",
       "      <td>2025-05-12 23:16:05</td>\n",
       "      <td>I am Tired of the Struggle</td>\n",
       "      <td>innovstars</td>\n",
       "      <td>38</td>\n",
       "      <td>136</td>\n",
       "      <td>0.99</td>\n",
       "      <td>I (28 f) am currently in debt and struggling, ...</td>\n",
       "      <td>ms2q92s</td>\n",
       "      <td>2025-05-13 11:47:52</td>\n",
       "      <td>innovstars</td>\n",
       "      <td>I have told him I would if I needed too, I am ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>1mbceud</td>\n",
       "      <td>2025-07-28 10:18:40</td>\n",
       "      <td>Poor people can't have both kids and money</td>\n",
       "      <td>Grouchy_Key4343</td>\n",
       "      <td>271</td>\n",
       "      <td>1160</td>\n",
       "      <td>0.96</td>\n",
       "      <td>The way I see things, if you were born poor an...</td>\n",
       "      <td>n5l2nii</td>\n",
       "      <td>2025-07-28 10:31:03</td>\n",
       "      <td>Comntnmama</td>\n",
       "      <td>My dad joined the Navy but got out and was poo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id         created_utc  \\\n",
       "2860  1lzybaq 2025-07-14 20:56:16   \n",
       "7660  197zvaf 2024-01-16 10:32:31   \n",
       "785   1mbceud 2025-07-28 10:18:40   \n",
       "3244  1kl71d8 2025-05-12 23:16:05   \n",
       "709   1mbceud 2025-07-28 10:18:40   \n",
       "\n",
       "                                                  title                author  \\\n",
       "2860  How are you guys affording everyday basic hygi...         MoodyMagicOwl   \n",
       "7660                                      prescriptions  Illustrious-Level858   \n",
       "785          Poor people can't have both kids and money       Grouchy_Key4343   \n",
       "3244                         I am Tired of the Struggle            innovstars   \n",
       "709          Poor people can't have both kids and money       Grouchy_Key4343   \n",
       "\n",
       "      n_comments  score  ratio  \\\n",
       "2860         154    250   0.99   \n",
       "7660           5      4   0.75   \n",
       "785          271   1160   0.96   \n",
       "3244          38    136   0.99   \n",
       "709          271   1160   0.96   \n",
       "\n",
       "                                                   text comment_id  \\\n",
       "2860  The title pretty much explains itself.\\n\\n I a...    n35efx3   \n",
       "7660                        how to get free script help    kibctie   \n",
       "785   The way I see things, if you were born poor an...    n5muot7   \n",
       "3244  I (28 f) am currently in debt and struggling, ...    ms2q92s   \n",
       "709   The way I see things, if you were born poor an...    n5l2nii   \n",
       "\n",
       "             comment_utc comment_author  \\\n",
       "2860 2025-07-14 21:24:01    Carrie_1968   \n",
       "7660 2024-01-17 18:27:33       kkaavvbb   \n",
       "785  2025-07-28 16:33:35      TheMaze01   \n",
       "3244 2025-05-13 11:47:52     innovstars   \n",
       "709  2025-07-28 10:31:03     Comntnmama   \n",
       "\n",
       "                                                   body  comment_score  \n",
       "2860  I do this. Once you get used to it you can *fe...              2  \n",
       "7660  Try calling the drug company. They typically o...              2  \n",
       "785       And you've sought no remedy over the decades?             -2  \n",
       "3244  I have told him I would if I needed too, I am ...              1  \n",
       "709   My dad joined the Navy but got out and was poo...              2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df['created_utc'] = pd.to_datetime(reddit_df['created_utc'], unit='s')\n",
    "reddit_df['comment_utc'] = pd.to_datetime(reddit_df['comment_utc'], unit='s')\n",
    "reddit_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From previous experiences with this dataset, I know when a comment is removed or deleted, the `body` text is replaced with `[deleted]` or `[removed]`.  I will take a look to see if the count of those comments is significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                74\n",
      "created_utc       74\n",
      "title             74\n",
      "author            74\n",
      "n_comments        74\n",
      "score             74\n",
      "ratio             74\n",
      "text              74\n",
      "comment_id        74\n",
      "comment_utc       74\n",
      "comment_author    74\n",
      "body              74\n",
      "comment_score     74\n",
      "dtype: int64\n",
      "id                75\n",
      "created_utc       75\n",
      "title             75\n",
      "author            75\n",
      "n_comments        75\n",
      "score             75\n",
      "ratio             75\n",
      "text              75\n",
      "comment_id        75\n",
      "comment_utc       75\n",
      "comment_author    75\n",
      "body              75\n",
      "comment_score     75\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(reddit_df[reddit_df['body'] == '[deleted]'].count())\n",
    "print(reddit_df[reddit_df['body'] == '[removed]'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number is not significant so I will drop these rows.  Its less than 1% of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                0\n",
      "created_utc       0\n",
      "title             0\n",
      "author            0\n",
      "n_comments        0\n",
      "score             0\n",
      "ratio             0\n",
      "text              0\n",
      "comment_id        0\n",
      "comment_utc       0\n",
      "comment_author    0\n",
      "body              0\n",
      "comment_score     0\n",
      "dtype: int64\n",
      "id                0\n",
      "created_utc       0\n",
      "title             0\n",
      "author            0\n",
      "n_comments        0\n",
      "score             0\n",
      "ratio             0\n",
      "text              0\n",
      "comment_id        0\n",
      "comment_utc       0\n",
      "comment_author    0\n",
      "body              0\n",
      "comment_score     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "indexBody = reddit_df[(reddit_df['body'] == '[deleted]') | \\\n",
    "                      (reddit_df['body'] =='[removed]')].index\n",
    "reddit_df.drop(indexBody, inplace=True)\n",
    "print(reddit_df[reddit_df['body'] == '[deleted]'].count())\n",
    "print(reddit_df[reddit_df['body'] == '[removed]'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears from the quick look at the dataset that the `text` feature may be null or blank values. I will check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reddit_df[\u001b[43mreddit_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m]\n",
      "File \u001b[0;32m~/regis/virtual/python3.13/lib64/python3.13/site-packages/pandas/core/ops/common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/regis/virtual/python3.13/lib64/python3.13/site-packages/pandas/core/arraylike.py:48\u001b[0m, in \u001b[0;36mOpsMixin.__lt__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__lt__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__lt__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/regis/virtual/python3.13/lib64/python3.13/site-packages/pandas/core/series.py:6119\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6116\u001b[0m lvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   6117\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 6119\u001b[0m res_values \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[0;32m~/regis/virtual/python3.13/lib64/python3.13/site-packages/pandas/core/ops/array_ops.py:344\u001b[0m, in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m lvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 344\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mcomp_method_OBJECT_ARRAY\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    347\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/regis/virtual/python3.13/lib64/python3.13/site-packages/pandas/core/ops/array_ops.py:129\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m    127\u001b[0m     result \u001b[38;5;241m=\u001b[39m libops\u001b[38;5;241m.\u001b[39mvec_compare(x\u001b[38;5;241m.\u001b[39mravel(), y\u001b[38;5;241m.\u001b[39mravel(), op)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mlibops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscalar_compare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32mops.pyx:107\u001b[0m, in \u001b[0;36mpandas._libs.ops.scalar_compare\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "reddit_df[reddit_df['text'] < 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 8110 entries, 0 to 8258\n",
      "Data columns (total 13 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   id              8110 non-null   object        \n",
      " 1   created_utc     8110 non-null   datetime64[ns]\n",
      " 2   title           8110 non-null   object        \n",
      " 3   author          8110 non-null   object        \n",
      " 4   n_comments      8110 non-null   int64         \n",
      " 5   score           8110 non-null   int64         \n",
      " 6   ratio           8110 non-null   float64       \n",
      " 7   text            8110 non-null   object        \n",
      " 8   comment_id      8110 non-null   object        \n",
      " 9   comment_utc     8110 non-null   datetime64[ns]\n",
      " 10  comment_author  8110 non-null   object        \n",
      " 11  body            8110 non-null   object        \n",
      " 12  comment_score   8110 non-null   int64         \n",
      "dtypes: datetime64[ns](2), float64(1), int64(3), object(7)\n",
      "memory usage: 887.0+ KB\n"
     ]
    }
   ],
   "source": [
    "reddit_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text\n",
       "262      1\n",
       "1045     7\n",
       "719      6\n",
       "2325     1\n",
       "74       2\n",
       "63       3\n",
       "176     11\n",
       "1453     5\n",
       "2226     6\n",
       "1381    10\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df['text'].map(len).value_counts().sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 336,775 rows the dataset is larger than required for the lab.  In addition, clustering with K-Means can be computational and time intensive.  I will truncate the dataset to a number that can be processed in a reasonable amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reddit_df = reddit_df.head(10000)\n",
    "reddit_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------\n",
    "### Preprocess Text\n",
    "\n",
    "In order to conduct an meaningful EDA of the text data, I will apply some basic NLTK preprocessing to it.  This includes removing punctuation, converting all to lower case, removing numbers, extra spaces, stop words, and breaking words down to their *lemmas*.  I have defined a function to carry out these tasks.  I am concentrating on the `title` and `body` features, but will also process `author` and `comment_author`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['title', 'body', 'author', 'comment_author']\n",
    "for col in cols:\n",
    "    reddit_df[col] = reddit_df[col].apply(process_text)\n",
    "reddit_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this study, I am mostly concern with the text data in the `title` and `body` features.  In order to make processing easier, I will merged the two.  I will also add in `author` and `comment_author` to add more words to be clustered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df['content'] = reddit_df['title'] + reddit_df['body'] + \\\n",
    "                       reddit_df['author'] + reddit_df['comment_author']\n",
    "reddit_df['content'].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------\n",
    "## Perform some Basic EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reddit_df.info())\n",
    "print('\\nDataset shape: ', reddit_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 10,000 rows and 13 columns.  There are no NaN values and the datatypes include *datetime64[ns]*, *int64*, *float64*, and *object*.  For this analysis, I am concentrating on *object* text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List the top 10 Words**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.Series(' '.join(reddit_df['content']).split()).value_counts(ascending=False).to_dict()\n",
    "print('Top 10 Words:')\n",
    "list(freq.items())[:10] # using list to make the output more readable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a WordCloud from Text Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(width=1000, height=600, max_words=500).generate_from_frequencies(freq)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word cloud along with the top 10 word list can give a person a really good idea of what SubReddit was downloaded.  The most used words such as *moon*, *space*, *earth*, and *astronaut* are really good indicators of the subject under discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorize Text Data**\n",
    "\n",
    "Machine Learning algorithms do not understand text information and in order to feed text into an ML algorithm, it must be converted to numeric values.  There are a few options available to do this task.  In a past lab we used a *CountVectorizer*, which simply counts the number of times a word appears in the document.  The *TfidfVectorizer*, on the other hand, counts the words and takes into account how important that word is to the whole corpus.  I've also added parameters suggested by this article [Sparse Features](https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py), since the dataset I am working with is sparse in nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(sublinear_tf=True, max_df=0.5, min_df=5, stop_words='english')\n",
    "X = tfv.fit_transform(reddit_df['content'])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------\n",
    "### Cluster Text with K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means clustering is one of the most popular unsupervised machine learning algorithms. K-Means clustering is used to find intrinsic groups within an unlabelled dataset and draw inferences from them. In this study, I will be clustering the most used words in the Reddit submissions.\n",
    "\n",
    "K-means optimizes a non-convex objective function and its clustering is not guaranteed to be optimal for a given random init.  Furthermore, sparse high-dimensional data such as text vectorized using the *Bag of Words* approach (which I am doing in this lab), can cause k-means to initialize centroids on extremely isolated data points.  One way to avoid this problem is to increase the number of runs with independent random initiators.  Increasing the `n_init` parameter's value will do this [Sparse Features](https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py).  The default for `n_init` changes depending on the value of `init`, since the default for `init` is *k-means++* the default for `n_init` is **1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare a list to store results in\n",
    "sum_sq= []\n",
    "\n",
    "# fit the model for a range of 1 to 11 clusters and add to sum_sq[]\n",
    "for n in range (1, 11):\n",
    "    print('Calculating for ',n,' clusters')\n",
    "    \n",
    "    # random_state makes the results more reproducible \n",
    "    km_model = KMeans(n_clusters=n, max_iter=2000, n_init=2, random_state=42)\n",
    "    km_model.fit(X)\n",
    "    sum_sq.append(-km_model.score(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One method to determine the optimal number of clusters is to plot the results of the K-Means clustering algorithm and looking for a bend in the plot.  The bend should indicate the optimal number of clusters.  This is known as the *elbow method* and the plot is known as a *scree plot*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, 11), sum_sq, 'bx-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot is a little difficult to interpret.  A bend starts at 5 with a sharper change at 6. Based on the angle of the curve at 6, I'll run the model with 6 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the KMeans model with 6 clusters\n",
    "km_model = KMeans(n_clusters=6, n_init=2, random_state=42)\n",
    "km_model.fit(X)\n",
    "\n",
    "# gather the predictions\n",
    "preds = km_model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use sklearn.metrics silhouette_score to gauge the performance of 5 clusters.  The Silhouette Coefficient is calculated using the mean intra-cluster distance and the nearest-cluster distance for each sample.  The best value is 1 and the worst is -1. Values near 0 indicate overlapping clusters [sklearn.metrics.silhoulette_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html).  I would like to see a score close to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = metrics.silhouette_score(X, preds, sample_size=2000)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score is above zero, but still not very close to 1.  I will try with 5 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the KMeans model with 5 clusters\n",
    "km_model = KMeans(n_clusters=5, n_init=2, random_state=42)\n",
    "km_model.fit(X)\n",
    "\n",
    "# gather the predictions\n",
    "preds = km_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = metrics.silhouette_score(X, preds, sample_size=2000)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The silhouette score for 5 clusters is closer to 1 than the 6 cluster model.  However, both scores are fairly close to 0 which indicates the clusters maybe overlapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In very high-dimensional spaces, euclidean distances tend to become inflated (curse of dimensionality). This may have happened here and to confirm I will check the inertia score. Inertia is the means of how internally coherent the clusters are. A score of zero is optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "km_model.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score is significantly higher than zero.  I will apply an algorithm to reduce the dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------\n",
    "**Principal Component Analysis (PCA)**\n",
    "\n",
    "It is possible the dataset had too many dimensions and this is causing the clustering algorithm to not perform as expected (*Curse of dimensionality*).  Linear dimensionality can be reduced using ***Singular Value Decomposition (SVD)***.  I will reduce the dimensions of the dataset and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the SVD object and reduce dataset\n",
    "svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
    "data_reduced = svd.fit_transform(X)\n",
    "data_reduced = pd.DataFrame(data_reduced)\n",
    "\n",
    "# Plot results\n",
    "ax = data_reduced.plot(kind='scatter', x=0, y=1, c=preds, cmap='rainbow')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_title('K-Mean Clusters after Reducing Dimensions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above indicates the clusters are not well defined.  Even though there is overlabp, the plot still shows 4 clusters.  There is overlap along the edges of the clusters and this confirms that the *inertia_ score* is indicating overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hierarchical Cluster Analysis (HCA)**\n",
    "\n",
    "**HCA** is an algorithm that can be used to make the clusters more distinct and provide better separation between them.  It works by treating each observation as a separate cluster.  Then it repeatedly executes the following two steps:\n",
    "1) Identify the two clusters that are closest together\n",
    "2) Merge the two most similar ones.  This continues until all the clusters are merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dendrograms**\n",
    "\n",
    "We can use a dendrogram to visualize the history of groupings and figure out the optimal number of clusters. To determine the largest vertical distance that does not intersect any of the other clusters Draw a horizontal line at both extremities The optimal number of clusters is equal to the number of vertical lines going through the horizontal line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I will select a sample of 2000 from the 10,000\n",
    "indices = np.random.choice(X.shape[0], 2000, replace=False)\n",
    "ydist = X[indices].toarray() # The linkage() method input requires an array\n",
    "\n",
    "# The scipy hierarchial linkage method is used to create the HCA\n",
    "# parameter ward instructs the method to use the Ward variance \n",
    "# minimization algorithm\n",
    "Z = linkage(ydist, 'ward')\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(figsize=(15, 8))\n",
    "dendrogram(Z)\n",
    "plt.title('Dendrogram on Sample Data')\n",
    "plt.ylabel('Euclidean distances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "dendrogram(Z)\n",
    "plt.title('Dendrogram')\n",
    "plt.ylabel('Eluclidean distances')\n",
    "plt.axhline(y=9, color='b', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I understand the Lecture for Week 6 correctly, the dendrogram is indicating I should apply hierarchical clustering for 5 clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will try K-Means again with 5 clusters on the data that had PCA and HCA applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit the KMeans model with 5 clusters\n",
    "km_model = KMeans(n_clusters=5, n_init=2, random_state=42)\n",
    "km_model.fit(Z)\n",
    "\n",
    "# gather the predictions\n",
    "preds = km_model.predict(Z)\n",
    "metrics.silhouette_score(Z, preds, sample_size=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the *inertia_ score*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "km_model.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The *silhouette score* shows significant improvement; therefore, the clusters are not overlapping as much as before the PCA.  Unfortunately, it looks likes the clusters are less defined after the HCA as the *inertia score* is significantly higher than before the HCA which indicates the clusters are not well defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------\n",
    "### Summary and Discussion\n",
    "**Summary**\n",
    "\n",
    "The goal of this lab was to download a text-based dataset from Twitter or Reddit.  I chose to download submittals from the Space subReddit.  The submitttal  and its comments had to be downloaded separately due to API limitations.  Since the relationship between a submittal and comments is a one-to-many, I saved the submittals and their comments to a *sqlite3* database file.  This allowed me to load the data into a dataframe using a *SQL* join statement.\n",
    "\n",
    "After creating the dataframe from a SQL statement, I performed some basic data wrangling to change the timestamps from UNIX epoch to a readable format.  I also identified the sumbittal's title and the comment's body as the features of interest in this study.  As part of the data wrangling process, I also reduced the number of records in the dataset from over 300,000 to 10,000 using Pandas' head function.  Then I preprocessed all of the text features by converting all characters to lowercase, removing digits, removing punctuation, and applying a lemmatizer algorithm to reduce words to their basic forms.  I chose to lemmatize the text instead of stemming, since sklearn's *WordNetLemmatizer* actually uses a corpra (word dictionary) to look up the words being lemmatized.  The basic form of the word is know as a *lemma*.  In this case, I believe the lemmatizer reduced *Mars* to *mar*.  this maybe due to me converting everything to lowercase before applying the lemmatizer.\n",
    "\n",
    "The features `title`, `body`, `author`, and `comment_author` were merged together into one feature named `content`. Furthermore, the `text` feature was dropped, since it did not contain any useful information.  After the merge and drop processes, some basic EDA was conducted on the feature.  This included listing the top 10 words used in the dataset and plotting a word cloud.  \n",
    "\n",
    "Next the dataset under study was vectorized using the TfidfVectorizer function.  the TfidfVectorizer converts a collection of raw documents to a matrix of TF-IDF features.  These features were then fitted to a K-Means algorithm and a 10 step loop was constructed to iterate through different number of clusters and a score calculated to find the optimal number of clusters to use.  Using a *scree* plot and the *silhouette score*, I determined that 6 clusters were the optimal number.  However, the *silhouette* and *inertia_* scores indicated that the clusters were not well defined and the model may be suffering from the *curse of dimentionality*.  Therefore, a SVD method was applied to the dataset to reduce its linear dimensionality.  None the less, a PCA plot indicated the clusters were still not well formed and a HCA was conducted.\n",
    "\n",
    "**Results**\n",
    "\n",
    "The PCA process did reduce the *dimentionality* of the clusters with a significantly larger score.  This indicates the clusters are still overlapping, but not as bad as before the PCA.  The *inertia score* after HCA indicates the clusters are still not well defined and actually became less defined.\n",
    "\n",
    "| Process                              | Silhouette | Inertia      |\n",
    "|--------------------------------------|------------|--------------|\n",
    "| K-Means<br> 5 Clusters                   | 0.063      | 8750.09      |\n",
    "| K-Means<br> 6 Clusters                   | 0.062      | Not Recorded |\n",
    "| K-Means after<br> PCA and HCA<br> 5 Clusters | 0.43      | 526322304.61  |\n",
    "\n",
    "**Discussion**\n",
    "\n",
    "I found that just clustering text is most likely not the best use for K-Means clustering.  However, I do think that this can be remedy by setting some categories for the clusters.  For example, try to cluster around categories such as *Earth*, *Moon*, *Mars*, and *Image*.  It would be interesting to see if K-Means could create those clusters based on analyzing the text of the submittal and its comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
