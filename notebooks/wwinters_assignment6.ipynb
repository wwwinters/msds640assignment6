{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------\n",
    "- Wiley Winters\n",
    "- MSDS 640 - Assignment 6 Social Media Analysis for the Common Good\n",
    "- 2025-AUG-16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "# Assignment Specification\n",
    "For this week's assignment, you are tasked with write report APA-formatted paper (3-4 pages long). Assume the role of a data science researcher employed at a non-profit organization, approaching the topic from a data science's perspective.\n",
    "- Your main objective is to utilize social media data to contribute to a common good issue. Choose a topic such as **mental health**, **income inequality**, **human rights**, **workers' rights**, a particular **healthcare concern**, or **socioeconomic injustices**. Select a social media platform for data collection, and options include Reddit, X (formerly Twitter), Facebook, or others\n",
    "- At a minimum create the wordcloud and include it in your paper. To go above and beyond, apply other NLP and text analytics techniques, such as [topic modeling](https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0) and sentiment analysis. Note that many people consider wordclouds uninformative and [bad practice](https://getthematic.com/insights/word-clouds-harm-insights/), so you should strive to create a bar chart of top wordcloud or other visualizations instead, which can be done using tools and examples provided in resources like \"MSDS640_Week6_FTE.ipynb\"\n",
    "- Your paper should also feature a mindmap. This mindmap should center around the common good issue you have selected, with social media platforms branching out from the center. Further layers can delve into ethics and privacy concerns related to the project, culminating in examples of these issues\n",
    "- In your work, include an overall ***introduction***, a ***description of your dataset***, the ***purpose** behind your research, highlighting the problem you seek to address, and a discussion on ethics and privacy challenges in the context of your chosen common good issue. Additionally, provide a summary of your findings. For further insights and inspiration, refer to the weekly reading list, which includes videos and mind-mapping resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------\n",
    "## Import Required Packages and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Imports\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Read sqlite3 database file\n",
    "import sqlite3\n",
    "\n",
    "# Text processing and preparation\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords, words\n",
    "\n",
    "# Text Visualization\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Sentiment Analysis\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Make plots pretty\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process text function to perform basic preprocessing on text features.  I changed this function to use lemmatization instead of stemming.  While stemming can be faster to perform, lemmatization actually reduces the word-forms to linguistically valid lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower() # Convert all to lower case\n",
    "        # Remove punctuation\n",
    "        text = ''.join([char for char in text if char not in string.punctuation])\n",
    "        text = ''.join([char for char in text if not char.isdigit()])  # Remove numbers\n",
    "        # In some instances, I've run into issues with extra spaces.\n",
    "        text = text.strip()\n",
    "        # Remove stop words and apply lemmatizer\n",
    "        stop = stopwords.words('english')\n",
    "        wnl = WordNetLemmatizer()\n",
    "        text = ' '.join([wnl.lemmatize(word) for word in text.split() if word not in \\\n",
    "                         stop])\n",
    "\n",
    "        return text\n",
    "    else:\n",
    "        return ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to download **Reddit** submissions and their associated comments.  Since the relationship between submissions and comments are one to many, I decided to store the data in a *sqlite3* datafile.  This will allow me to query the data using standard SQL statements that allows for one to many SQL joins.\n",
    "\n",
    "The code below creates a connection to the datafile and then queries it based on criteria I have chosen.  A pandas dataframe is created from the query.\n",
    "\n",
    "Since I believe the purpose of the lab is to perform a sentiment analysis, I will only load text features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>n_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>ratio</th>\n",
       "      <th>text</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_utc</th>\n",
       "      <th>comment_author</th>\n",
       "      <th>body</th>\n",
       "      <th>comment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5875</th>\n",
       "      <td>1m36veg</td>\n",
       "      <td>1.752856e+09</td>\n",
       "      <td>How can I make $75 quick?</td>\n",
       "      <td>PsychologicalWait189</td>\n",
       "      <td>41</td>\n",
       "      <td>17</td>\n",
       "      <td>0.87</td>\n",
       "      <td></td>\n",
       "      <td>n3y2unm</td>\n",
       "      <td>1.752902e+09</td>\n",
       "      <td>MoodyMagicOwl</td>\n",
       "      <td>Have you done this? If so, do these guys care ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2233</th>\n",
       "      <td>1kzyuez</td>\n",
       "      <td>1.748703e+09</td>\n",
       "      <td>Prominent conservative attacks Social Security...</td>\n",
       "      <td>Socialfilterdvit</td>\n",
       "      <td>205</td>\n",
       "      <td>356</td>\n",
       "      <td>0.96</td>\n",
       "      <td></td>\n",
       "      <td>mvrfk1o</td>\n",
       "      <td>1.748955e+09</td>\n",
       "      <td>EffectiveSalamander</td>\n",
       "      <td>He's admitting that privatization would destro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8049</th>\n",
       "      <td>ooiue4</td>\n",
       "      <td>1.626842e+09</td>\n",
       "      <td>Question for r/poverty</td>\n",
       "      <td>ozzy622</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>If any of you are living in poverty, have live...</td>\n",
       "      <td>h60m28i</td>\n",
       "      <td>1.626883e+09</td>\n",
       "      <td>excaligirltoo</td>\n",
       "      <td>And this isn’t even just a poverty situation. ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5181</th>\n",
       "      <td>1mlxqvc</td>\n",
       "      <td>1.754767e+09</td>\n",
       "      <td>Feeling like a failure</td>\n",
       "      <td>Secret-Requirement22</td>\n",
       "      <td>31</td>\n",
       "      <td>25</td>\n",
       "      <td>1.00</td>\n",
       "      <td>I feel like a failure and don’t know how to ge...</td>\n",
       "      <td>n7uphey</td>\n",
       "      <td>1.754781e+09</td>\n",
       "      <td>Substantial-Use-1758</td>\n",
       "      <td>Community college, girl. You’ve still got 30+ ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4397</th>\n",
       "      <td>1iofed5</td>\n",
       "      <td>1.739439e+09</td>\n",
       "      <td>Favorite Poverty Meals?</td>\n",
       "      <td>Intrepid-Opening5877</td>\n",
       "      <td>59</td>\n",
       "      <td>56</td>\n",
       "      <td>0.99</td>\n",
       "      <td>And I’m talking like DIRT cheap, as low as you...</td>\n",
       "      <td>mgqnog5</td>\n",
       "      <td>1.741466e+09</td>\n",
       "      <td>NurseCrystal81</td>\n",
       "      <td>Do you follow Dollar Tree Meals on TikTok?  Sh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id   created_utc  \\\n",
       "5875  1m36veg  1.752856e+09   \n",
       "2233  1kzyuez  1.748703e+09   \n",
       "8049   ooiue4  1.626842e+09   \n",
       "5181  1mlxqvc  1.754767e+09   \n",
       "4397  1iofed5  1.739439e+09   \n",
       "\n",
       "                                                  title                author  \\\n",
       "5875                          How can I make $75 quick?  PsychologicalWait189   \n",
       "2233  Prominent conservative attacks Social Security...      Socialfilterdvit   \n",
       "8049                             Question for r/poverty               ozzy622   \n",
       "5181                             Feeling like a failure  Secret-Requirement22   \n",
       "4397                            Favorite Poverty Meals?  Intrepid-Opening5877   \n",
       "\n",
       "      n_comments  score  ratio  \\\n",
       "5875          41     17   0.87   \n",
       "2233         205    356   0.96   \n",
       "8049          15      3   1.00   \n",
       "5181          31     25   1.00   \n",
       "4397          59     56   0.99   \n",
       "\n",
       "                                                   text comment_id  \\\n",
       "5875                                                       n3y2unm   \n",
       "2233                                                       mvrfk1o   \n",
       "8049  If any of you are living in poverty, have live...    h60m28i   \n",
       "5181  I feel like a failure and don’t know how to ge...    n7uphey   \n",
       "4397  And I’m talking like DIRT cheap, as low as you...    mgqnog5   \n",
       "\n",
       "       comment_utc        comment_author  \\\n",
       "5875  1.752902e+09         MoodyMagicOwl   \n",
       "2233  1.748955e+09   EffectiveSalamander   \n",
       "8049  1.626883e+09         excaligirltoo   \n",
       "5181  1.754781e+09  Substantial-Use-1758   \n",
       "4397  1.741466e+09        NurseCrystal81   \n",
       "\n",
       "                                                   body  comment_score  \n",
       "5875  Have you done this? If so, do these guys care ...              2  \n",
       "2233  He's admitting that privatization would destro...              1  \n",
       "8049  And this isn’t even just a poverty situation. ...              2  \n",
       "5181  Community college, girl. You’ve still got 30+ ...              7  \n",
       "4397  Do you follow Dollar Tree Meals on TikTok?  Sh...              1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = sqlite3.connect('data/poverty.sqlite')  # Create the database connection object\n",
    "\n",
    "reddit_df = pd.read_sql_query('SELECT id, created_utc, title, author, n_comments, ' \\\n",
    "                              'score, ratio, text, comment_id, comment_utc, ' \\\n",
    "                              'comment_author, body, comment_score ' \\\n",
    "                              'FROM posts, comments WHERE posts.id = comments.link_id ' \\\n",
    "                              'AND n_comments > 0', conn)\n",
    "# Take a quick peek at the data\n",
    "reddit_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dates are in Unix Epoch.  I will convert them into something a little more human readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>n_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>ratio</th>\n",
       "      <th>text</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_utc</th>\n",
       "      <th>comment_author</th>\n",
       "      <th>body</th>\n",
       "      <th>comment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1m91bdh</td>\n",
       "      <td>2025-07-25 14:54:23</td>\n",
       "      <td>Poor people are taught shame for the things th...</td>\n",
       "      <td>CarpenterUpset3251</td>\n",
       "      <td>180</td>\n",
       "      <td>3364</td>\n",
       "      <td>0.99</td>\n",
       "      <td>This is just a thought. But I feel like it's s...</td>\n",
       "      <td>n56yorv</td>\n",
       "      <td>2025-07-26 01:31:45</td>\n",
       "      <td>Elegant_Break9371</td>\n",
       "      <td>Ah. That explains it lol</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5879</th>\n",
       "      <td>1m36veg</td>\n",
       "      <td>2025-07-18 16:26:32</td>\n",
       "      <td>How can I make $75 quick?</td>\n",
       "      <td>PsychologicalWait189</td>\n",
       "      <td>41</td>\n",
       "      <td>17</td>\n",
       "      <td>0.87</td>\n",
       "      <td></td>\n",
       "      <td>n43g55g</td>\n",
       "      <td>2025-07-20 01:45:51</td>\n",
       "      <td>team_undog</td>\n",
       "      <td>I was just kidding. I would care if I was into...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3198</th>\n",
       "      <td>1lh6cfo</td>\n",
       "      <td>2025-06-21 20:43:21</td>\n",
       "      <td>To people who make fun of us poor people: I'm ...</td>\n",
       "      <td>Different_Lychee9708</td>\n",
       "      <td>71</td>\n",
       "      <td>146</td>\n",
       "      <td>0.92</td>\n",
       "      <td>I am officially done. From now on, ANYONE who ...</td>\n",
       "      <td>mz31ljt</td>\n",
       "      <td>2025-06-22 01:49:00</td>\n",
       "      <td>Diane1967</td>\n",
       "      <td>It’s really hurtful when people use our posts ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1673</th>\n",
       "      <td>1m3jg9t</td>\n",
       "      <td>2025-07-19 01:11:14</td>\n",
       "      <td>The struggle is real.</td>\n",
       "      <td>Apprehensive_Snow45</td>\n",
       "      <td>273</td>\n",
       "      <td>876</td>\n",
       "      <td>0.98</td>\n",
       "      <td>I'm fresh out of prison and I'm feeling overwh...</td>\n",
       "      <td>n4s9eet</td>\n",
       "      <td>2025-07-23 21:03:23</td>\n",
       "      <td>Few-Reason7527</td>\n",
       "      <td>Find a church , they will help guide you. Have...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1939</th>\n",
       "      <td>1k3zhs8</td>\n",
       "      <td>2025-04-20 23:29:10</td>\n",
       "      <td>What the rich eat !</td>\n",
       "      <td>Sweet-Leadership-290</td>\n",
       "      <td>158</td>\n",
       "      <td>468</td>\n",
       "      <td>0.95</td>\n",
       "      <td>I am disgusted at the waste of resources. I ha...</td>\n",
       "      <td>moozjog</td>\n",
       "      <td>2025-04-23 23:00:40</td>\n",
       "      <td>greenerbeansheen</td>\n",
       "      <td>KIRKLAND!!!!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id         created_utc  \\\n",
       "92    1m91bdh 2025-07-25 14:54:23   \n",
       "5879  1m36veg 2025-07-18 16:26:32   \n",
       "3198  1lh6cfo 2025-06-21 20:43:21   \n",
       "1673  1m3jg9t 2025-07-19 01:11:14   \n",
       "1939  1k3zhs8 2025-04-20 23:29:10   \n",
       "\n",
       "                                                  title                author  \\\n",
       "92    Poor people are taught shame for the things th...    CarpenterUpset3251   \n",
       "5879                          How can I make $75 quick?  PsychologicalWait189   \n",
       "3198  To people who make fun of us poor people: I'm ...  Different_Lychee9708   \n",
       "1673                              The struggle is real.   Apprehensive_Snow45   \n",
       "1939                                What the rich eat !  Sweet-Leadership-290   \n",
       "\n",
       "      n_comments  score  ratio  \\\n",
       "92           180   3364   0.99   \n",
       "5879          41     17   0.87   \n",
       "3198          71    146   0.92   \n",
       "1673         273    876   0.98   \n",
       "1939         158    468   0.95   \n",
       "\n",
       "                                                   text comment_id  \\\n",
       "92    This is just a thought. But I feel like it's s...    n56yorv   \n",
       "5879                                                       n43g55g   \n",
       "3198  I am officially done. From now on, ANYONE who ...    mz31ljt   \n",
       "1673  I'm fresh out of prison and I'm feeling overwh...    n4s9eet   \n",
       "1939  I am disgusted at the waste of resources. I ha...    moozjog   \n",
       "\n",
       "             comment_utc     comment_author  \\\n",
       "92   2025-07-26 01:31:45  Elegant_Break9371   \n",
       "5879 2025-07-20 01:45:51         team_undog   \n",
       "3198 2025-06-22 01:49:00          Diane1967   \n",
       "1673 2025-07-23 21:03:23     Few-Reason7527   \n",
       "1939 2025-04-23 23:00:40   greenerbeansheen   \n",
       "\n",
       "                                                   body  comment_score  \n",
       "92                             Ah. That explains it lol              2  \n",
       "5879  I was just kidding. I would care if I was into...              2  \n",
       "3198  It’s really hurtful when people use our posts ...              5  \n",
       "1673  Find a church , they will help guide you. Have...              1  \n",
       "1939                                       KIRKLAND!!!!              1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df['created_utc'] = pd.to_datetime(reddit_df['created_utc'], unit='s')\n",
    "reddit_df['comment_utc'] = pd.to_datetime(reddit_df['comment_utc'], unit='s')\n",
    "reddit_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From previous experiences with this dataset, I know when a comment is removed or deleted, the `body` text is replaced with `[deleted]` or `[removed]`.  I will take a look to see if the count of those comments is significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                74\n",
      "created_utc       74\n",
      "title             74\n",
      "author            74\n",
      "n_comments        74\n",
      "score             74\n",
      "ratio             74\n",
      "text              74\n",
      "comment_id        74\n",
      "comment_utc       74\n",
      "comment_author    74\n",
      "body              74\n",
      "comment_score     74\n",
      "dtype: int64\n",
      "id                75\n",
      "created_utc       75\n",
      "title             75\n",
      "author            75\n",
      "n_comments        75\n",
      "score             75\n",
      "ratio             75\n",
      "text              75\n",
      "comment_id        75\n",
      "comment_utc       75\n",
      "comment_author    75\n",
      "body              75\n",
      "comment_score     75\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(reddit_df[reddit_df['body'] == '[deleted]'].count())\n",
    "print(reddit_df[reddit_df['body'] == '[removed]'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number is not significant so I will drop these rows.  Its less than 1% of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                0\n",
      "created_utc       0\n",
      "title             0\n",
      "author            0\n",
      "n_comments        0\n",
      "score             0\n",
      "ratio             0\n",
      "text              0\n",
      "comment_id        0\n",
      "comment_utc       0\n",
      "comment_author    0\n",
      "body              0\n",
      "comment_score     0\n",
      "dtype: int64\n",
      "id                0\n",
      "created_utc       0\n",
      "title             0\n",
      "author            0\n",
      "n_comments        0\n",
      "score             0\n",
      "ratio             0\n",
      "text              0\n",
      "comment_id        0\n",
      "comment_utc       0\n",
      "comment_author    0\n",
      "body              0\n",
      "comment_score     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "indexBody = reddit_df[(reddit_df['body'] == '[deleted]') | \\\n",
    "                      (reddit_df['body'] =='[removed]')].index\n",
    "reddit_df.drop(indexBody, inplace=True)\n",
    "print(reddit_df[reddit_df['body'] == '[deleted]'].count())\n",
    "print(reddit_df[reddit_df['body'] == '[removed]'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears from the quick look at the dataset that the `text` feature may be null or blank values. I will check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text\n",
       "0       701\n",
       "2258    355\n",
       "36      296\n",
       "2004    294\n",
       "554     270\n",
       "       ... \n",
       "686       1\n",
       "710       1\n",
       "4134      1\n",
       "342       1\n",
       "51        1\n",
       "Name: count, Length: 434, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df['text'].map(len).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appears the `text` feature does not contain text.  I will drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df.drop('text', axis=1, inplace=True)\n",
    "reddit_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 336,775 rows the dataset is larger than required for the lab.  In addition, clustering with K-Means can be computational and time intensive.  I will truncate the dataset to a number that can be processed in a reasonable amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reddit_df = reddit_df.head(10000)\n",
    "reddit_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------\n",
    "### Preprocess Text\n",
    "\n",
    "In order to conduct an meaningful EDA of the text data, I will apply some basic NLTK preprocessing to it.  This includes removing punctuation, converting all to lower case, removing numbers, extra spaces, stop words, and breaking words down to their *lemmas*.  I have defined a function to carry out these tasks.  I am concentrating on the `title` and `body` features, but will also process `author` and `comment_author`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['title', 'body', 'author', 'comment_author']\n",
    "for col in cols:\n",
    "    reddit_df[col] = reddit_df[col].apply(process_text)\n",
    "reddit_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this study, I am mostly concern with the text data in the `title` and `body` features.  In order to make processing easier, I will merged the two.  I will also add in `author` and `comment_author` to add more words to be clustered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df['content'] = reddit_df['title'] + reddit_df['body'] + \\\n",
    "                       reddit_df['author'] + reddit_df['comment_author']\n",
    "reddit_df['content'].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------\n",
    "## Perform some Basic EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reddit_df.info())\n",
    "print('\\nDataset shape: ', reddit_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 10,000 rows and 13 columns.  There are no NaN values and the datatypes include *datetime64[ns]*, *int64*, *float64*, and *object*.  For this analysis, I am concentrating on *object* text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List the top 10 Words**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.Series(' '.join(reddit_df['content']).split()).value_counts(ascending=False).to_dict()\n",
    "print('Top 10 Words:')\n",
    "list(freq.items())[:10] # using list to make the output more readable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a WordCloud from Text Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(width=1000, height=600, max_words=500).generate_from_frequencies(freq)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word cloud along with the top 10 word list can give a person a really good idea of what SubReddit was downloaded.  The most used words such as *moon*, *space*, *earth*, and *astronaut* are really good indicators of the subject under discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorize Text Data**\n",
    "\n",
    "Machine Learning algorithms do not understand text information and in order to feed text into an ML algorithm, it must be converted to numeric values.  There are a few options available to do this task.  In a past lab we used a *CountVectorizer*, which simply counts the number of times a word appears in the document.  The *TfidfVectorizer*, on the other hand, counts the words and takes into account how important that word is to the whole corpus.  I've also added parameters suggested by this article [Sparse Features](https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py), since the dataset I am working with is sparse in nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(sublinear_tf=True, max_df=0.5, min_df=5, stop_words='english')\n",
    "X = tfv.fit_transform(reddit_df['content'])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------\n",
    "### Cluster Text with K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means clustering is one of the most popular unsupervised machine learning algorithms. K-Means clustering is used to find intrinsic groups within an unlabelled dataset and draw inferences from them. In this study, I will be clustering the most used words in the Reddit submissions.\n",
    "\n",
    "K-means optimizes a non-convex objective function and its clustering is not guaranteed to be optimal for a given random init.  Furthermore, sparse high-dimensional data such as text vectorized using the *Bag of Words* approach (which I am doing in this lab), can cause k-means to initialize centroids on extremely isolated data points.  One way to avoid this problem is to increase the number of runs with independent random initiators.  Increasing the `n_init` parameter's value will do this [Sparse Features](https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py).  The default for `n_init` changes depending on the value of `init`, since the default for `init` is *k-means++* the default for `n_init` is **1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare a list to store results in\n",
    "sum_sq= []\n",
    "\n",
    "# fit the model for a range of 1 to 11 clusters and add to sum_sq[]\n",
    "for n in range (1, 11):\n",
    "    print('Calculating for ',n,' clusters')\n",
    "    \n",
    "    # random_state makes the results more reproducible \n",
    "    km_model = KMeans(n_clusters=n, max_iter=2000, n_init=2, random_state=42)\n",
    "    km_model.fit(X)\n",
    "    sum_sq.append(-km_model.score(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One method to determine the optimal number of clusters is to plot the results of the K-Means clustering algorithm and looking for a bend in the plot.  The bend should indicate the optimal number of clusters.  This is known as the *elbow method* and the plot is known as a *scree plot*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, 11), sum_sq, 'bx-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot is a little difficult to interpret.  A bend starts at 5 with a sharper change at 6. Based on the angle of the curve at 6, I'll run the model with 6 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the KMeans model with 6 clusters\n",
    "km_model = KMeans(n_clusters=6, n_init=2, random_state=42)\n",
    "km_model.fit(X)\n",
    "\n",
    "# gather the predictions\n",
    "preds = km_model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use sklearn.metrics silhouette_score to gauge the performance of 5 clusters.  The Silhouette Coefficient is calculated using the mean intra-cluster distance and the nearest-cluster distance for each sample.  The best value is 1 and the worst is -1. Values near 0 indicate overlapping clusters [sklearn.metrics.silhoulette_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html).  I would like to see a score close to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = metrics.silhouette_score(X, preds, sample_size=2000)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score is above zero, but still not very close to 1.  I will try with 5 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the KMeans model with 5 clusters\n",
    "km_model = KMeans(n_clusters=5, n_init=2, random_state=42)\n",
    "km_model.fit(X)\n",
    "\n",
    "# gather the predictions\n",
    "preds = km_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = metrics.silhouette_score(X, preds, sample_size=2000)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The silhouette score for 5 clusters is closer to 1 than the 6 cluster model.  However, both scores are fairly close to 0 which indicates the clusters maybe overlapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In very high-dimensional spaces, euclidean distances tend to become inflated (curse of dimensionality). This may have happened here and to confirm I will check the inertia score. Inertia is the means of how internally coherent the clusters are. A score of zero is optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "km_model.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score is significantly higher than zero.  I will apply an algorithm to reduce the dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------\n",
    "**Principal Component Analysis (PCA)**\n",
    "\n",
    "It is possible the dataset had too many dimensions and this is causing the clustering algorithm to not perform as expected (*Curse of dimensionality*).  Linear dimensionality can be reduced using ***Singular Value Decomposition (SVD)***.  I will reduce the dimensions of the dataset and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the SVD object and reduce dataset\n",
    "svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
    "data_reduced = svd.fit_transform(X)\n",
    "data_reduced = pd.DataFrame(data_reduced)\n",
    "\n",
    "# Plot results\n",
    "ax = data_reduced.plot(kind='scatter', x=0, y=1, c=preds, cmap='rainbow')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_title('K-Mean Clusters after Reducing Dimensions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above indicates the clusters are not well defined.  Even though there is overlabp, the plot still shows 4 clusters.  There is overlap along the edges of the clusters and this confirms that the *inertia_ score* is indicating overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hierarchical Cluster Analysis (HCA)**\n",
    "\n",
    "**HCA** is an algorithm that can be used to make the clusters more distinct and provide better separation between them.  It works by treating each observation as a separate cluster.  Then it repeatedly executes the following two steps:\n",
    "1) Identify the two clusters that are closest together\n",
    "2) Merge the two most similar ones.  This continues until all the clusters are merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dendrograms**\n",
    "\n",
    "We can use a dendrogram to visualize the history of groupings and figure out the optimal number of clusters. To determine the largest vertical distance that does not intersect any of the other clusters Draw a horizontal line at both extremities The optimal number of clusters is equal to the number of vertical lines going through the horizontal line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I will select a sample of 2000 from the 10,000\n",
    "indices = np.random.choice(X.shape[0], 2000, replace=False)\n",
    "ydist = X[indices].toarray() # The linkage() method input requires an array\n",
    "\n",
    "# The scipy hierarchial linkage method is used to create the HCA\n",
    "# parameter ward instructs the method to use the Ward variance \n",
    "# minimization algorithm\n",
    "Z = linkage(ydist, 'ward')\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(figsize=(15, 8))\n",
    "dendrogram(Z)\n",
    "plt.title('Dendrogram on Sample Data')\n",
    "plt.ylabel('Euclidean distances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "dendrogram(Z)\n",
    "plt.title('Dendrogram')\n",
    "plt.ylabel('Eluclidean distances')\n",
    "plt.axhline(y=9, color='b', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I understand the Lecture for Week 6 correctly, the dendrogram is indicating I should apply hierarchical clustering for 5 clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will try K-Means again with 5 clusters on the data that had PCA and HCA applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit the KMeans model with 5 clusters\n",
    "km_model = KMeans(n_clusters=5, n_init=2, random_state=42)\n",
    "km_model.fit(Z)\n",
    "\n",
    "# gather the predictions\n",
    "preds = km_model.predict(Z)\n",
    "metrics.silhouette_score(Z, preds, sample_size=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the *inertia_ score*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "km_model.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The *silhouette score* shows significant improvement; therefore, the clusters are not overlapping as much as before the PCA.  Unfortunately, it looks likes the clusters are less defined after the HCA as the *inertia score* is significantly higher than before the HCA which indicates the clusters are not well defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------\n",
    "### Summary and Discussion\n",
    "**Summary**\n",
    "\n",
    "The goal of this lab was to download a text-based dataset from Twitter or Reddit.  I chose to download submittals from the Space subReddit.  The submitttal  and its comments had to be downloaded separately due to API limitations.  Since the relationship between a submittal and comments is a one-to-many, I saved the submittals and their comments to a *sqlite3* database file.  This allowed me to load the data into a dataframe using a *SQL* join statement.\n",
    "\n",
    "After creating the dataframe from a SQL statement, I performed some basic data wrangling to change the timestamps from UNIX epoch to a readable format.  I also identified the sumbittal's title and the comment's body as the features of interest in this study.  As part of the data wrangling process, I also reduced the number of records in the dataset from over 300,000 to 10,000 using Pandas' head function.  Then I preprocessed all of the text features by converting all characters to lowercase, removing digits, removing punctuation, and applying a lemmatizer algorithm to reduce words to their basic forms.  I chose to lemmatize the text instead of stemming, since sklearn's *WordNetLemmatizer* actually uses a corpra (word dictionary) to look up the words being lemmatized.  The basic form of the word is know as a *lemma*.  In this case, I believe the lemmatizer reduced *Mars* to *mar*.  this maybe due to me converting everything to lowercase before applying the lemmatizer.\n",
    "\n",
    "The features `title`, `body`, `author`, and `comment_author` were merged together into one feature named `content`. Furthermore, the `text` feature was dropped, since it did not contain any useful information.  After the merge and drop processes, some basic EDA was conducted on the feature.  This included listing the top 10 words used in the dataset and plotting a word cloud.  \n",
    "\n",
    "Next the dataset under study was vectorized using the TfidfVectorizer function.  the TfidfVectorizer converts a collection of raw documents to a matrix of TF-IDF features.  These features were then fitted to a K-Means algorithm and a 10 step loop was constructed to iterate through different number of clusters and a score calculated to find the optimal number of clusters to use.  Using a *scree* plot and the *silhouette score*, I determined that 6 clusters were the optimal number.  However, the *silhouette* and *inertia_* scores indicated that the clusters were not well defined and the model may be suffering from the *curse of dimentionality*.  Therefore, a SVD method was applied to the dataset to reduce its linear dimensionality.  None the less, a PCA plot indicated the clusters were still not well formed and a HCA was conducted.\n",
    "\n",
    "**Results**\n",
    "\n",
    "The PCA process did reduce the *dimentionality* of the clusters with a significantly larger score.  This indicates the clusters are still overlapping, but not as bad as before the PCA.  The *inertia score* after HCA indicates the clusters are still not well defined and actually became less defined.\n",
    "\n",
    "| Process                              | Silhouette | Inertia      |\n",
    "|--------------------------------------|------------|--------------|\n",
    "| K-Means<br> 5 Clusters                   | 0.063      | 8750.09      |\n",
    "| K-Means<br> 6 Clusters                   | 0.062      | Not Recorded |\n",
    "| K-Means after<br> PCA and HCA<br> 5 Clusters | 0.43      | 526322304.61  |\n",
    "\n",
    "**Discussion**\n",
    "\n",
    "I found that just clustering text is most likely not the best use for K-Means clustering.  However, I do think that this can be remedy by setting some categories for the clusters.  For example, try to cluster around categories such as *Earth*, *Moon*, *Mars*, and *Image*.  It would be interesting to see if K-Means could create those clusters based on analyzing the text of the submittal and its comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
